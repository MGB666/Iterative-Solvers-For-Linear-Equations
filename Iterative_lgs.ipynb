{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d2c95e2",
   "metadata": {},
   "source": [
    "# Iterative Solvers for LGSe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d52692",
   "metadata": {},
   "source": [
    "## Jacobi-Method  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb20495",
   "metadata": {},
   "source": [
    "The Jacobi Method is a form of fixed point iteration for a system of equations . The first step of the Jacobi Method is; Solves the $i$th equation for the $i$th unknown . Then, iterate as in Fixed-Point-Iteration , starting with initial guess . \n",
    "\n",
    "Let $A$ be strictly diagonally dominant (nxn) matrix . $D$ is the main diagonal of A and $U$ denote upper triangle and $L$ denote lower triangle . Then $ A = L + D + U $ . \n",
    "+ The Jacobi Method converges to unique solution . \n",
    "\n",
    "+ The system of equations can be rearranged in a fixed-point of form :    \n",
    "    \n",
    "$$ Ax = b $$\n",
    "$$(D + L + U)x = b $$\n",
    "$$ Dx = b - (L + U)x $$\n",
    "$$ x = D^{-1} (b - (L + U)x) $$\n",
    "\n",
    "$$ x_0 = initial vector $$ \n",
    "\n",
    "$$ x_{k+1} = D^{-1}(b - (L + U)x_k) $$  for $$ k = 0,1,2,3,... $$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f6ffed",
   "metadata": {},
   "source": [
    "### Jacobi-Method implementation with Python \n",
    "To calculate jacobi method in python I have created jacobi function with while loop.\n",
    "\n",
    "+ First I have defined matrix A = (mxn) , then to initialise number of iterations I defined k = 1 \n",
    "+ I have defined starting vector wich takes elements of zeros . \n",
    "+ While loop continues until its smaller or equal to the max numeber of iterations. \n",
    "+ To calculate error I used np.linalg.norm. \n",
    "+ If error is smaller than tol it returns X0 . After that function incrementes k and updates parameter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8db6f5",
   "metadata": {},
   "source": [
    "$ note: $ I have saved my spyder file as iterative_löser.py rather numla.py . I have tried to save it as numla.py but I had probem when ever I tried to imported, it couldnt find the file so I changed the file name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd51401",
   "metadata": {},
   "source": [
    "#### Testing iterative_löser.py importing with jacobi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aed38078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lösüng nach 100 Iterationsschritten : [3. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import iterative_löser as il\n",
    "\n",
    "A = np.array([[4 ,-1 , 1] , [-1 , 4 , -2] , [1 , -2 , 4]])\n",
    "b = np.array([[12] , [-1] , [5]])\n",
    "x = np.ones(shape = 3)\n",
    "    \n",
    "(x,its) = il.jacobi (A , b , tol = 1.e-8 )\n",
    "\n",
    "\n",
    "print ('Lösüng nach {} Iterationsschritten : {}' .format (its,x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec0c1bd",
   "metadata": {},
   "source": [
    "## Gauß Seidel-Method "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b3bc7d",
   "metadata": {},
   "source": [
    "Jacobi and Gauß Seidel are closely related . The only difference between Gauß Seidel and Jacobi is that in the former , the most recently updated values of the unknowns are used at each step , even if the updating occurs in the current step.\n",
    "\n",
    "$$ (L + D + U)x = b $$\n",
    "$$ (L + D )x_{k+1} = -Ux_k + b $$\n",
    "\n",
    "$$ x_0 = initial vector $$\n",
    "\n",
    "$$ x_{k+1} = D^{-1} (b - U_{x_k} - L_{x_k+1}) $$ for $$ k = 0,1,2,3,... $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b535b859",
   "metadata": {},
   "source": [
    "### Gauß Seidel implementation with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cb86f3",
   "metadata": {},
   "source": [
    "+ Gauß Seidel and Jacobi functions are mostly the same only difference is that While loop has another for -> (i+1,n) statement which allowes to calculate upated values ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1afdca",
   "metadata": {},
   "source": [
    "#### Testing iterative_löser.py importing with Gauß Seidel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3002450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lösüng nach 100 Iterationsschritten : [3. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import iterative_löser as il\n",
    "\n",
    "A = np.array([[4 ,-1 , 1] , [-1 , 4 , -2] , [1 , -2 , 4]])\n",
    "b = np.array([[12] , [-1] , [5]])\n",
    "x = np.ones(shape = 3)\n",
    "    \n",
    "(x,its) = il.gauss_seidel (A , b , tol = 1.e-8 )\n",
    "\n",
    "\n",
    "print ('Lösüng nach {} Iterationsschritten : {}' .format (its,x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858ba1d3",
   "metadata": {},
   "source": [
    "## SOR-Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6674118a",
   "metadata": {},
   "source": [
    "SOR takes Gauß Seidel direction toward the solution and overshoots to try to speed convergence . The number $ω$ is called relaxation parameter.\n",
    "+ $ω > 1$ is referred to as over-relaxation\n",
    "+ $ω < 1$ is referred to as under-relaxation\n",
    "+ SOR with $ω = 1$ is exactly Gauß Seidel .\n",
    "\n",
    "$$ (ωL + ωD + ωU)x = ωb $$\n",
    "$$ (ωL + D)x = ωb - ωU_x + (1 - ω)D_x $$\n",
    "\n",
    "$$ x_0 = initial vector $$\n",
    "\n",
    "$$ x_{k+1} = (ωL + D)^{-1} [(1 - ω)D_{x_k} - ωU_{x_k}] + ω(D + ωL )^{-1}b $$\n",
    "\n",
    "for\n",
    "\n",
    "$$ k = 0,1,2,3,... $$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9cdfdd",
   "metadata": {},
   "source": [
    "### SOR implementation with Python "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a75904",
   "metadata": {},
   "source": [
    "Gauß Seidel and Jacobi functions are mostly the same . To implement sor in this function there is w which is omega . SOR implemented to the initial vector . Thus allows sor method to converge faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b365423a",
   "metadata": {},
   "source": [
    "#### Testing iterative_löser.py importing with SOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c096fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lösüng nach 100 Iterationsschritten : [3. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import iterative_löser as il\n",
    "\n",
    "A = np.array([[4 ,-1 , 1] , [-1 , 4 , -2] , [1 , -2 , 4]])\n",
    "b = np.array([[12] , [-1] , [5]])\n",
    "x = np.ones(shape = 3)\n",
    "    \n",
    "(x,its) = il.sor (A , b , tol = 1.e-8 , w = 1. )\n",
    "\n",
    "\n",
    "print ('Lösüng nach {} Iterationsschritten : {}' .format (its,x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1cc589",
   "metadata": {},
   "source": [
    "# 2-b Checking the expected error reduction per iteration step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb86733",
   "metadata": {},
   "source": [
    "## Error reduction - Jacobi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a6143a",
   "metadata": {},
   "source": [
    "To show error reduction in Jacobi method I added new print statement which allowes me to show number of iterations and amount of error at each step . Before the error value reaches 0.0 it stops and outputs the converged result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "60f1888c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def jacobi(A , b , X0 = None , tol = 1.e-8 , max_n = 100):\n",
    "    \n",
    "    (m,n)= A.shape\n",
    "    \n",
    "    #Initialising number of iterations\n",
    "    k = 1\n",
    "    #Define starting vector \n",
    "    if X0 == None:\n",
    "        X0 = np.zeros(shape = n)\n",
    "        nIt = 100\n",
    "        x = X0.copy()\n",
    "        \n",
    "        #Loop to calculate jacobi\n",
    "        \n",
    "    while k <= max_n :\n",
    "        for i in range(n):\n",
    "            s = 0\n",
    "            for j in range(n):\n",
    "                if(i == j):\n",
    "                    continue\n",
    "                else:\n",
    "                        s += A[i][j] * x[j]\n",
    "                X0[i] = (-s + b[i]) / (A[i][i])\n",
    "                        \n",
    "                        #this print statements shows number of iterations and amount of error at each step . \n",
    "                        #before the error value reaches 0.0 it stops and outputs the converged result \n",
    "                        print ('iteration',k,'\\n','\\t error', np.linalg.norm(X0-x))\n",
    "                        \n",
    "                        #Calculates error , here I used residual norm (x_currentstep-x_previousstep)\n",
    "                        err = np.linalg.norm(X0 - x)\n",
    "                        #Checks error condition\n",
    "                        if (err < tol):\n",
    "                            return X0 , k\n",
    "                        #Incrementing k\n",
    "                        k += 1\n",
    "                        #Updating parameter\n",
    "                        for i in range(n):\n",
    "                            x[i] = X0[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a012eb62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 \n",
      " \t error 3.0\n",
      "iteration 2 \n",
      " \t error 0.5\n",
      "iteration 3 \n",
      " \t error 2.25\n",
      "iteration 4 \n",
      " \t error 1.75\n",
      "iteration 5 \n",
      " \t error 0.25\n",
      "iteration 6 \n",
      " \t error 0.125\n",
      "iteration 7 \n",
      " \t error 0.03125\n",
      "iteration 8 \n",
      " \t error 1.546875\n",
      "iteration 9 \n",
      " \t error 1.828125\n",
      "iteration 10 \n",
      " \t error 0.265625\n",
      "iteration 11 \n",
      " \t error 0.0078125\n",
      "iteration 12 \n",
      " \t error 0.001953125\n",
      "iteration 13 \n",
      " \t error 1.5654296875\n",
      "iteration 14 \n",
      " \t error 1.8330078125\n",
      "iteration 15 \n",
      " \t error 0.2666015625\n",
      "iteration 16 \n",
      " \t error 0.00048828125\n",
      "iteration 17 \n",
      " \t error 0.0001220703125\n",
      "iteration 18 \n",
      " \t error 1.56658935546875\n",
      "iteration 19 \n",
      " \t error 1.83331298828125\n",
      "iteration 20 \n",
      " \t error 0.26666259765625\n",
      "iteration 21 \n",
      " \t error 3.0517578125e-05\n",
      "iteration 22 \n",
      " \t error 7.62939453125e-06\n",
      "iteration 23 \n",
      " \t error 1.5666618347167969\n",
      "iteration 24 \n",
      " \t error 1.8333320617675781\n",
      "iteration 25 \n",
      " \t error 0.2666664123535156\n",
      "iteration 26 \n",
      " \t error 1.9073486328125e-06\n",
      "iteration 27 \n",
      " \t error 4.76837158203125e-07\n",
      "iteration 28 \n",
      " \t error 1.5666663646697998\n",
      "iteration 29 \n",
      " \t error 1.8333332538604736\n",
      "iteration 30 \n",
      " \t error 0.2666666507720947\n",
      "iteration 31 \n",
      " \t error 1.1920928955078125e-07\n",
      "iteration 32 \n",
      " \t error 2.9802322387695312e-08\n",
      "iteration 33 \n",
      " \t error 1.5666666477918625\n",
      "iteration 34 \n",
      " \t error 1.8333333283662796\n",
      "iteration 35 \n",
      " \t error 0.2666666656732559\n",
      "iteration 36 \n",
      " \t error 7.450580596923828e-09\n",
      "Lösüng nach 36 Iterationsschritten : [3.13333333 0.53333333 0.73333333]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[4 ,-1 , 1] , [-1 , 4 , -2] , [1 , -2 , 4]])\n",
    "b = np.array([[12] , [-1] , [5]])\n",
    "x = np.ones(shape = 3)\n",
    "    \n",
    "(x,its) = jacobi(A , b , tol = 1.e-8 )\n",
    "    \n",
    "print ('Lösüng nach {} Iterationsschritten : {}' .format (its,x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75e22f4",
   "metadata": {},
   "source": [
    "## Error reduction - Gauß Seidel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d0efeff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss_seidel (A , b , X0 = None , tol = 1.e-8 , max_n = 100):\n",
    "    \n",
    "    (m,n) = A.shape \n",
    "    \n",
    "    # Initialising number of iteration\n",
    "    k = 1\n",
    "    #Define starting vector \n",
    "    if X0 == None:\n",
    "        X0 = np.zeros(shape = n)\n",
    "        nIt = 100\n",
    "        x = X0.copy()\n",
    "        \n",
    "        #Loop to calculate Gauss_Seidel\n",
    "    while k <= max_n :\n",
    "        for i in range (n):\n",
    "            s = 0\n",
    "            for j in range (i):\n",
    "                if (i==j):\n",
    "                    continue\n",
    "                else:\n",
    "                    s += A[i][j]*X0[j]\n",
    "            for j in range (i + 1 , n):\n",
    "                if (i==j):\n",
    "                    continue\n",
    "                else:\n",
    "                    s += A[i][j]*x[j]\n",
    "            X0[i] = (-s + b[i]) / (A[i][i])\n",
    "            \n",
    "            #this print statements shows number of iterations and amount of error at each step . \n",
    "            #before the error value reaches 0.0 it stops and outputs the converged result \n",
    "            print ('iteration',k,'\\n','\\t error', np.linalg.norm(X0-x))\n",
    "\n",
    "            #Calculates error , here I used residual norm (x_currentstep-x_previousstep)\n",
    "            err = np.linalg.norm(X0 - x)\n",
    "            #Checks error condition\n",
    "            if (err < tol):\n",
    "                return x , k\n",
    "            #Incrementing k\n",
    "            k = k+1\n",
    "            #Updating parameter\n",
    "            for i in range(n):\n",
    "                x[i] = X0[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4d14a478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 \n",
      " \t error 3.0\n",
      "iteration 2 \n",
      " \t error 0.5\n",
      "iteration 3 \n",
      " \t error 0.75\n",
      "iteration 4 \n",
      " \t error 0.0625\n",
      "iteration 5 \n",
      " \t error 0.359375\n",
      "iteration 6 \n",
      " \t error 0.1953125\n",
      "iteration 7 \n",
      " \t error 0.041015625\n",
      "iteration 8 \n",
      " \t error 0.10791015625\n",
      "iteration 9 \n",
      " \t error 0.043701171875\n",
      "iteration 10 \n",
      " \t error 0.01605224609375\n",
      "iteration 11 \n",
      " \t error 0.0258636474609375\n",
      "iteration 12 \n",
      " \t error 0.00891876220703125\n",
      "iteration 13 \n",
      " \t error 0.0042362213134765625\n",
      "iteration 14 \n",
      " \t error 0.005518436431884766\n",
      "iteration 15 \n",
      " \t error 0.0017001628875732422\n",
      "iteration 16 \n",
      " \t error 0.0009545683860778809\n",
      "iteration 17 \n",
      " \t error 0.0010887235403060913\n",
      "iteration 18 \n",
      " \t error 0.00030571967363357544\n",
      "iteration 19 \n",
      " \t error 0.00019575096666812897\n",
      "iteration 20 \n",
      " \t error 0.00020179757848381996\n",
      "iteration 21 \n",
      " \t error 5.196104757487774e-05\n",
      "iteration 22 \n",
      " \t error 3.7459132727235556e-05\n",
      "iteration 23 \n",
      " \t error 3.534530696924776e-05\n",
      "iteration 24 \n",
      " \t error 8.30787030281499e-06\n",
      "iteration 25 \n",
      " \t error 6.759359166608192e-06\n",
      "iteration 26 \n",
      " \t error 5.843774943059543e-06\n",
      "iteration 27 \n",
      " \t error 1.2320476798777236e-06\n",
      "iteration 28 \n",
      " \t error 1.152931815795455e-06\n",
      "iteration 29 \n",
      " \t error 9.042567938877255e-07\n",
      "iteration 30 \n",
      " \t error 1.6389544299499903e-07\n",
      "iteration 31 \n",
      " \t error 1.8509033772318162e-07\n",
      "iteration 32 \n",
      " \t error 1.2822030592829492e-07\n",
      "iteration 33 \n",
      " \t error 1.7837568533352055e-08\n",
      "iteration 34 \n",
      " \t error 2.7595684404246867e-08\n",
      "iteration 35 \n",
      " \t error 1.5817705367737744e-08\n",
      "iteration 36 \n",
      " \t error 1.0099314717848529e-09\n",
      "Lösüng nach 36 Iterationsschritten : [3. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[4 ,-1 , 1] , [-1 , 4 , -2] , [1 , -2 , 4]])\n",
    "b = np.array([[12] , [-1] , [5]])\n",
    "x = np.ones(shape = 3)\n",
    "    \n",
    "(x,its) = gauss_seidel (A , b , tol = 1.e-8 )\n",
    "    \n",
    "print ('Lösüng nach {} Iterationsschritten : {}' .format (its,x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7adfc1",
   "metadata": {},
   "source": [
    "## Error reduction - SOR "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de84a39e",
   "metadata": {},
   "source": [
    "for $ omega = 1 $ convergence is the same as Gauß Seidel but if we take $ omega = 1.1 $  it converges faster . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "10c52a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def sor (A , b , X0 = None , tol = 1.e-8 , max_n = 100, w = 1.5):\n",
    "    \n",
    "    (m,n) = A.shape \n",
    "    \n",
    "    # Initialising number of iteration\n",
    "    k = 1\n",
    "    #Define starting vector \n",
    "    if X0 == None:\n",
    "        X0 = np.zeros(shape = n)\n",
    "        nIt = 100\n",
    "        x = X0.copy()\n",
    "        #omega\n",
    "        w = 1.5\n",
    "        \n",
    "        #Loop to calculate sor\n",
    "    while k <= max_n :\n",
    "        for i in range (n):\n",
    "            s = 0\n",
    "            for j in range (i):\n",
    "                if (i==j):\n",
    "                    continue\n",
    "                else:\n",
    "                    s += A[i][j]*X0[j]\n",
    "            for j in range (i + 1 , n):\n",
    "                if (i==j):\n",
    "                    continue\n",
    "                else:\n",
    "                    s += A[i][j]*x[j]\n",
    "                    \n",
    "            # SOR implementation\n",
    "            X0[i] = (1-w) * X0[i] + (w / A[i][i]) * (b[i]-s)\n",
    "            \n",
    "            #this print statements shows number of iterations and amount of error at each step . \n",
    "            #before the error value reaches 0.0 it stops and outputs the converged result \n",
    "            print ('iteration',k,'\\n','\\t error', np.linalg.norm(X0-x))\n",
    "\n",
    "            \n",
    "            #Calculates error , here I used residual norm (x_currentstep-x_previousstep)\n",
    "            err = np.linalg.norm(X0 - x)\n",
    "            #Checks error condition\n",
    "            if (err < tol):\n",
    "                return x , k\n",
    "            #Incrementing k\n",
    "            k += 1\n",
    "            #Updating parameter\n",
    "            for i in range(n):\n",
    "                x[i] = X0[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "04daa0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 \n",
      " \t error 4.5\n",
      "iteration 2 \n",
      " \t error 1.3125\n",
      "iteration 3 \n",
      " \t error 1.171875\n",
      "iteration 4 \n",
      " \t error 2.197265625\n",
      "iteration 5 \n",
      " \t error 0.601318359375\n",
      "iteration 6 \n",
      " \t error 0.21295166015625\n",
      "iteration 7 \n",
      " \t error 0.9529953002929688\n",
      "iteration 8 \n",
      " \t error 0.4983186721801758\n",
      "iteration 9 \n",
      " \t error 0.12284159660339355\n",
      "iteration 10 \n",
      " \t error 0.33569374680519104\n",
      "iteration 11 \n",
      " \t error 0.28291329368948936\n",
      "iteration 12 \n",
      " \t error 0.14772061351686716\n",
      "iteration 13 \n",
      " \t error 0.1171496183378622\n",
      "iteration 14 \n",
      " \t error 0.07459729358379263\n",
      "iteration 15 \n",
      " \t error 0.08587717006957973\n",
      "iteration 16 \n",
      " \t error 0.06280476285110126\n",
      "iteration 17 \n",
      " \t error 0.003557444691125511\n",
      "iteration 18 \n",
      " \t error 0.01671871544728276\n",
      "iteration 19 \n",
      " \t error 0.03900594147745373\n",
      "iteration 20 \n",
      " \t error 0.0003094691230203228\n",
      "iteration 21 \n",
      " \t error 0.006035768488138471\n",
      "iteration 22 \n",
      " \t error 0.017123506634542096\n",
      "iteration 23 \n",
      " \t error 0.011102875915567467\n",
      "iteration 24 \n",
      " \t error 0.0011120422953468934\n",
      "iteration 25 \n",
      " \t error 0.003981158988178635\n",
      "iteration 26 \n",
      " \t error 0.007878404299861086\n",
      "iteration 27 \n",
      " \t error 0.003859847456655463\n",
      "iteration 28 \n",
      " \t error 0.00048362067788776386\n",
      "iteration 29 \n",
      " \t error 0.001225674311646996\n",
      "iteration 30 \n",
      " \t error 0.002667821707855067\n",
      "iteration 31 \n",
      " \t error 0.0007826156125214645\n",
      "iteration 32 \n",
      " \t error 0.001094548270372142\n",
      "iteration 33 \n",
      " \t error 0.00021951879645276673\n",
      "iteration 34 \n",
      " \t error 0.0008840829563196984\n",
      "iteration 35 \n",
      " \t error 0.00038038212390578696\n",
      "iteration 36 \n",
      " \t error 0.0005070583033229825\n",
      "iteration 37 \n",
      " \t error 0.0003945379108780678\n",
      "iteration 38 \n",
      " \t error 0.00033805438211842453\n",
      "iteration 39 \n",
      " \t error 0.00014794008165197603\n",
      "iteration 40 \n",
      " \t error 1.5021031524842243e-05\n",
      "iteration 41 \n",
      " \t error 0.0002856151391198436\n",
      "iteration 42 \n",
      " \t error 0.00013460842669221762\n",
      "iteration 43 \n",
      " \t error 4.9117001398535365e-05\n",
      "iteration 44 \n",
      " \t error 2.343237401614129e-05\n",
      "iteration 45 \n",
      " \t error 0.00010329736938263778\n",
      "iteration 46 \n",
      " \t error 5.390872562127669e-06\n",
      "iteration 47 \n",
      " \t error 6.373526281833186e-05\n",
      "iteration 48 \n",
      " \t error 1.8256603668831417e-06\n",
      "iteration 49 \n",
      " \t error 2.7280782474381482e-05\n",
      "iteration 50 \n",
      " \t error 2.3006583256490742e-05\n",
      "iteration 51 \n",
      " \t error 2.657240068670852e-05\n",
      "iteration 52 \n",
      " \t error 1.2303209699915385e-05\n",
      "iteration 53 \n",
      " \t error 1.3039712524087754e-05\n",
      "iteration 54 \n",
      " \t error 8.120119587728958e-06\n",
      "iteration 55 \n",
      " \t error 1.7833321925841972e-06\n",
      "iteration 56 \n",
      " \t error 1.1941196380704788e-05\n",
      "iteration 57 \n",
      " \t error 5.5645870641329864e-06\n",
      "iteration 58 \n",
      " \t error 3.2828945903951023e-06\n",
      "iteration 59 \n",
      " \t error 5.660724211598023e-07\n",
      "iteration 60 \n",
      " \t error 4.437933319612064e-06\n",
      "iteration 61 \n",
      " \t error 1.8949945834734194e-07\n",
      "iteration 62 \n",
      " \t error 3.1164760758706223e-06\n",
      "iteration 63 \n",
      " \t error 4.732809999463683e-08\n",
      "iteration 64 \n",
      " \t error 1.0561807610720564e-06\n",
      "iteration 65 \n",
      " \t error 1.1266741772875122e-06\n",
      "iteration 66 \n",
      " \t error 1.2647374683094625e-06\n",
      "iteration 67 \n",
      " \t error 4.7631664568115184e-07\n",
      "iteration 68 \n",
      " \t error 5.638347546632616e-07\n",
      "iteration 69 \n",
      " \t error 3.8811141034322816e-07\n",
      "iteration 70 \n",
      " \t error 1.1882148909236889e-07\n",
      "iteration 71 \n",
      " \t error 5.284428763463467e-07\n",
      "iteration 72 \n",
      " \t error 2.4683451049778427e-07\n",
      "iteration 73 \n",
      " \t error 1.650138812259172e-07\n",
      "iteration 74 \n",
      " \t error 1.7215349701338312e-08\n",
      "iteration 75 \n",
      " \t error 1.982089729013481e-07\n",
      "iteration 76 \n",
      " \t error 1.4634331524376876e-08\n",
      "iteration 77 \n",
      " \t error 1.4553692928576112e-07\n",
      "iteration 78 \n",
      " \t error 4.560336108738738e-09\n",
      "Lösüng nach 78 Iterationsschritten : [2.99999998 1.00000006 1.00000003]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[4 ,-1 , 1] , [-1 , 4 , -2] , [1 , -2 , 4]])\n",
    "b = np.array([[12] , [-1] , [5]])\n",
    "x = np.ones(shape = 3)\n",
    "    \n",
    "(x,its) = sor (A , b , tol = 1.e-8 , w = 1.5)\n",
    "    \n",
    "print ('Lösüng nach {} Iterationsschritten : {}' .format (its,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cafcedf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
